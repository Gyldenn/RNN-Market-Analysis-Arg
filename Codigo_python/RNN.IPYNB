{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb1ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c54442",
   "metadata": {},
   "source": [
    "Inicio la clase con los datos y los pesos iniciales de la red en random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7e897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOnline(nn.Module):\n",
    "    def __init__(self, input_size=14, hidden_size = 256):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=3, dropout=0.1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # Copia del input para no modificar tensores en el grafo\n",
    "        x = x.clone()\n",
    "\n",
    "        #cambio la escala temporal de las variables en nanosegundos\n",
    "        x[:,:,0] = x[:,:,0] / 1e7\n",
    "        x[:,:,1] = x[:,:,1] / 1e7\n",
    "\n",
    "        #normalizo cierto datos del input con los valores que obtuve del robustScale de sckikit-learn. \n",
    "        #Elijo estos valores ya que son los que a mayor diferencia de escala se encuentran. En diferencias no tan grandes la propia red aprende la escala sin problemas\n",
    "        #volumen bid y offer\n",
    "        x[:,:,4] = (x[:,:,4]-573690) / 515196\n",
    "        x[:,:,5] = (x[:,:,5]-573690) / 515196\n",
    "\n",
    "        #price imbalance\n",
    "        x[:,:,11] = (x[:,:,11]+1.6963328e-06) / 8.28752812e-06\n",
    "\n",
    "        #depth ratio\n",
    "        x[:,:,12] = (x[:,:,12]+120097) / 494227\n",
    "\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240b622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = ['time_since_last_trade',\n",
    "                             'time_since_last_update',\n",
    "                             'spread',\n",
    "                             'volumen_bid',\n",
    "                             'volumen_off',\n",
    "                             'imbalance',\n",
    "                             'vwap_bid',\n",
    "                             'vwap_offer',\n",
    "                             'relative_spread',\n",
    "                             'log_spread',\n",
    "                             'price_imbalance',\n",
    "                             'depth_ratio',\n",
    "                             'direccion_bid',\n",
    "                             'direccion_offer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datos = pd.read_csv('Data_MEP')\n",
    "Datos.pop('mid_price')  #está a otra escala, lo uso para calcular los log_returns pero no para predecir.\n",
    "\n",
    "X = Datos.iloc[:, :-1].values.astype(np.float32)\n",
    "y = Datos.iloc[:, -1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b632dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (436133, 100, 14) secuencias, target: (436133,)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - seq_len):\n",
    "    X_seq.append(X[i:i+seq_len])\n",
    "    y_seq.append(y[i+seq_len-1])   # target: último paso de la secuencia\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(f\"Dataset: {X_seq.shape} secuencias, target: {y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd570815",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Convertir a tensores\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train).unsqueeze(1)  # (N, 1)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8894a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=False)  # online-like\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73c5afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNOnline(hidden_size=64)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "losses_train = []\n",
    "losses_train_epoch = []\n",
    "losses_test_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = None\n",
    "\n",
    "    for i, (xb, yb) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out, hidden = model(xb, hidden)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        if i % (len(train_loader)//100) == 0:\n",
    "            pct = 100 * i / len(train_loader)\n",
    "            losses_train.append(loss.item())\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - {pct:.1f}% completado\")\n",
    "            print(f\"loss_train: {loss}\")\n",
    "\n",
    "        if i % (len(train_loader)//10) == 0:\n",
    "            torch.save(model.state_dict(), f'checkpoints/Checkpoint_epoch{epoch}_{i}')\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    losses_train_epoch.append(avg_train_loss)\n",
    "\n",
    "    # VALIDACIÓN\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0\n",
    "        hidden = None\n",
    "        for xb, yb in test_loader:\n",
    "            out, hidden = model(xb, hidden)\n",
    "            loss = criterion(out, yb)\n",
    "            total_test_loss += loss.item()\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        losses_test_epoch.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.6f} | Test Loss: {avg_test_loss:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(losses_train, label=\"Train\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628663f",
   "metadata": {},
   "source": [
    "Exporto el modelo entrenado a c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f4cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo mayor alfabéticamente: checkpoints/Checkpoint_epoch1_0\n"
     ]
    }
   ],
   "source": [
    "#Elijo el ultimo modelo\n",
    "from pathlib import Path\n",
    "\n",
    "def archivo_mayor_alfabeticamente(carpeta_path):\n",
    "    carpeta = Path(carpeta_path)\n",
    "    \n",
    "    # Filtrar solo archivos y ordenarlos alfabéticamente\n",
    "    archivos = sorted([f for f in carpeta.iterdir() if f.is_file()])\n",
    "\n",
    "    if not archivos:\n",
    "        return None \n",
    "    \n",
    "    # El último archivo en orden alfabético\n",
    "    return archivos[-1]\n",
    "\n",
    "# Ejemplo de uso\n",
    "path = archivo_mayor_alfabeticamente(\"checkpoints\")\n",
    "print(\"Archivo mayor alfabéticamente:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d9ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNOnline(hidden_size=64)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "scripted = torch.jit.script(model)\n",
    "scripted.save(\"../Codigo_cpp/Modelo_entrenado.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
